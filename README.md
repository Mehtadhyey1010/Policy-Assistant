# ğŸ¤– LexoraAI - Intelligent Policy Assistant

**LexoraAI** is an intelligent document analysis and policy assistant powered by **RAG (Retrieval-Augmented Generation)** technology. Upload company policies, personnel manuals, or any documents, then ask natural language questions to get instant, context-aware answers.

> âœ¨ Transform your policy documents into an intelligent conversational assistant!

---

## ğŸ“‹ Features

### Core Capabilities
- **ğŸ“„ Document Upload** - Support for PDF and TXT files
- **ğŸ” Semantic Search** - Uses vector embeddings for intelligent document retrieval
- **ğŸ’¬ Q&A System** - Ask natural language questions about your documents
- **ğŸ“Š Auto Summarization** - Generate section-wise summaries of uploaded documents
- **ğŸ¯ Source Attribution** - Know exactly which document your answers come from
- **âš¡ Fast Retrieval** - FAISS vector database for millisecond search

### Advanced Features
- **ğŸ”„ Incremental Indexing** - Only new documents are processed (faster uploads)
- **ğŸ“š Multi-Document Support** - Search across all uploaded documents simultaneously
- **ğŸ§  Context-Aware Responses** - LLM generates answers only from document content
- **ğŸ” API-First Design** - RESTful API for easy integration

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    LEXORAAI SYSTEM                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Frontend (React + Vite)                                    â”‚
â”‚  â”œâ”€â”€ Document Upload UI                                    â”‚
â”‚  â”œâ”€â”€ Chat Interface                                        â”‚
â”‚  â”œâ”€â”€ Source References Panel                              â”‚
â”‚  â””â”€â”€ Responsive Dark Theme                                â”‚
â”‚                                                              â”‚
â”‚            â†“ HTTP/REST API (Port 8001)                     â”‚
â”‚                                                              â”‚
â”‚  Backend (FastAPI + Python)                                â”‚
â”‚  â”œâ”€â”€ Document Loading (PDF/TXT)                           â”‚
â”‚  â”œâ”€â”€ Text Chunking & Preprocessing                        â”‚
â”‚  â”œâ”€â”€ Vector Embedding Generation                          â”‚
â”‚  â”œâ”€â”€ FAISS Indexing & Search                              â”‚
â”‚  â””â”€â”€ LLM Integration (Groq API)                           â”‚
â”‚                                                              â”‚
â”‚            â†“                                                 â”‚
â”‚                                                              â”‚
â”‚  Data Storage                                               â”‚
â”‚  â”œâ”€â”€ data/raw/ (Original documents)                        â”‚
â”‚  â””â”€â”€ vectorstore/faiss_index/ (Vector database)            â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ›  Tech Stack

### Frontend
- **React 19** - UI Framework
- **Vite** - Build tool (fast development server)
- **Tailwind CSS** - Styling
- **JavaScript (ES6+)** - Client logic

### Backend
- **Python 3.11** - Core language
- **FastAPI** - Web framework
- **Uvicorn** - ASGI server
- **LangChain** - RAG pipeline orchestration
- **FAISS** - Vector similarity search
- **Sentence-Transformers** - Embeddings (all-MiniLM-L6-v2)
- **PyPDF** - PDF parsing
- **Groq API** - LLM (gpt-oss-120b)

### Infrastructure
- **Local Development** - Windows compatible
- **Port 5173** - Frontend (Vite)
- **Port 8001** - Backend API

---

## ğŸ“¦ Installation

### Prerequisites
- **Python 3.11+** 
- **Node.js 16+**
- **Groq API Key** (get free at https://console.groq.com)

### Backend Setup

```bash
cd rag-backend

# Install dependencies
pip install -r requirements.txt

# Create .env file with your Groq API key
echo "GROQ_API_KEY=your_api_key_here" > .env

# Build FAISS index (if documents exist)
python -c "from indexer.build_index import run_indexing_pipeline; run_indexing_pipeline()"
```

### Frontend Setup

```bash
cd rag-frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

---

## ğŸš€ Running LexoraAI

### Start Backend
```bash
cd rag-backend
$env:PYTHONPATH = "."
python -m uvicorn api.app:app --host 127.0.0.1 --port 8001
```

### Start Frontend
```bash
cd rag-frontend
npm run dev
```

### Access the Application
- **Frontend**: http://localhost:5173
- **Backend API**: http://127.0.0.1:8001
- **API Docs**: http://127.0.0.1:8001/docs (Swagger)

---

## ğŸ“š API Endpoints

### Upload Document
```http
POST /upload-document
Content-Type: multipart/form-data

Body:
  file: <PDF or TXT file>

Response:
  {
    "message": "âœ… Document uploaded successfully",
    "filename": "policy.pdf",
    "path": "data/raw/policy.pdf",
    "note": "File saved. You can ask questions about it."
  }
```

### Ask Question
```http
POST /ask
Content-Type: application/json

Body:
  {
    "question": "What is the leave policy?"
  }

Response:
  {
    "answer": "Based on the documents...",
    "sources": [
      {
        "source": "Comprehensive_Company_Policy_Manual.pdf",
        "page": 1
      }
    ]
  }
```

### Get Summary
```http
GET /summarize

Response:
  {
    "summary": "## Company Policies\n- Leave: 20 days annual...\n- Attendance: 85% required..."
  }
```

### Health Check
```http
GET /

Response:
  {
    "status": "RAG backend running"
  }
```

---

## ğŸ“ Project Structure

```
Policy-Assistant/
â”œâ”€â”€ README.md
â”œâ”€â”€ rag-backend/                          # Python FastAPI backend
â”‚   â”œâ”€â”€ main.py                           # Entry point
â”‚   â”œâ”€â”€ requirements.txt                  # Dependencies
â”‚   â”œâ”€â”€ .env                              # API keys
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ app.py                        # FastAPI application
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ settings.py                   # Configuration
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â””â”€â”€ raw/                          # Uploaded documents
â”‚   â”œâ”€â”€ embeddings/
â”‚   â”‚   â””â”€â”€ embedder.py                   # Embedding generation
â”‚   â”œâ”€â”€ indexer/
â”‚   â”‚   â””â”€â”€ build_index.py                # FAISS indexing (with incremental support)
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â””â”€â”€ groq_llm.py                   # Groq API integration
â”‚   â”œâ”€â”€ loaders/
â”‚   â”‚   â”œâ”€â”€ pdf_loader.py                 # PDF parsing
â”‚   â”‚   â””â”€â”€ txt_loader.py                 # TXT parsing
â”‚   â”œâ”€â”€ pipelines/
â”‚   â”‚   â”œâ”€â”€ rag_pipeline.py               # Q&A pipeline
â”‚   â”‚   â””â”€â”€ summarization_pipeline.py     # Summarization pipeline
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â””â”€â”€ chunker.py                    # Text chunking
â”‚   â”œâ”€â”€ retrieval/
â”‚   â”‚   â””â”€â”€ rag_retriever.py              # Vector search
â”‚   â””â”€â”€ vectorstore/
â”‚       â”œâ”€â”€ faiss_store.py                # FAISS operations
â”‚       â””â”€â”€ faiss_index/                  # Vector database
â”‚
â””â”€â”€ rag-frontend/                         # React Vite frontend
    â”œâ”€â”€ package.json
    â”œâ”€â”€ vite.config.js
    â”œâ”€â”€ index.html
    â”œâ”€â”€ src/
    â”‚   â”œâ”€â”€ main.jsx
    â”‚   â”œâ”€â”€ App.jsx
    â”‚   â”œâ”€â”€ App.css
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â””â”€â”€ ragApi.js                 # Backend API calls
    â”‚   â”œâ”€â”€ components/
    â”‚   â”‚   â”œâ”€â”€ ChatWindow.jsx             # Main chat interface
    â”‚   â”‚   â”œâ”€â”€ InputBox.jsx               # Question input
    â”‚   â”‚   â”œâ”€â”€ MessageBubble.jsx          # Chat message display
    â”‚   â”‚   â”œâ”€â”€ PdfUploader.jsx            # Document upload
    â”‚   â”‚   â””â”€â”€ SourcePanel.jsx            # Source references
    â”‚   â”œâ”€â”€ pages/
    â”‚   â”‚   â””â”€â”€ ChatPage.jsx               # Main page
    â”‚   â””â”€â”€ styles/
    â””â”€â”€ public/
```

---

## ğŸ”„ How It Works

### Data Pipeline

```
1. UPLOAD DOCUMENT
   Document â†’ Upload to /upload-document â†’ Saved in data/raw/

2. INDEXING (Automatic on Upload)
   PDFs/TXTs â†’ Load â†’ Chunk (500 char) â†’ Embed (384D vectors) â†’ FAISS Index
   
3. SEARCH (When User Asks Question)
   Question â†’ Embed â†’ FAISS Similarity Search (top 3) â†’ Retrieve Context
   
4. GENERATION (LLM Answer)
   Context + Question â†’ Groq LLM â†’ Answer
```

### Incremental Indexing

- **First Upload**: Builds complete FAISS index
- **Subsequent Uploads**: Only processes new documents
- **Tracking**: `indexed_files.json` tracks processed files
- **Performance**: 50-100x faster on repeated uploads

---

## ğŸ” Environment Variables

Create `.env` file in `rag-backend/`:

```env
# Required: Groq API Key
GROQ_API_KEY=gsk_xxxxxxxxxxxxxxxxxxxx

# Optional: Configuration
RAW_DATA_DIR=data/raw
VECTOR_DB_DIR=vectorstore/faiss_index
CHUNK_SIZE=500
CHUNK_OVERLAP=100
EMBEDDING_MODEL_NAME=all-MiniLM-L6-v2
```

---

## ğŸ“Š Data Storage

| Component | Location | Size | Format | Purpose |
|-----------|----------|------|--------|---------|
| Original Documents | `data/raw/` | ~10-50MB | PDF/TXT | Keep originals |
| Vector Index | `faiss_index/index.faiss` | ~90MB+ | Binary | Fast search |
| Metadata | `faiss_index/index.pkl` | ~2MB | Pickle | Index metadata |
| File Tracking | `indexed_files.json` | <1KB | JSON | Incremental indexing |

---

## ğŸ”§ Troubleshooting

### PyTorch DLL Error (Windows)
```bash
pip uninstall torch -y
pip install torch==2.0.1 --index-url https://download.pytorch.org/whl/cpu
```

### FAISS Index Not Found
```bash
# Rebuild index
cd rag-backend
python -c "from indexer.build_index import run_indexing_pipeline; run_indexing_pipeline()"
```

### Port Already in Use
```bash
# Change port in backend (api/app.py, line 135)
# Or kill existing process:
netstat -ano | Select-String ":8001" | ForEach-Object { taskkill /PID $_.Split()[4] /F }
```

---

## ğŸ¯ Usage Examples

### Example 1: Upload Policy Document
1. Click upload button
2. Select `Comprehensive_Company_Policy_Manual.pdf`
3. System indexes 69 pages in seconds

### Example 2: Ask About Leave Policy
```
User: "What is the annual leave policy?"
LexoraAI: "According to the Company Policy Manual, annual leave is 20 days 
          per employee..."
Source: Comprehensive_Company_Policy_Manual.pdf (Page 5)
```

### Example 3: Get Summary
```
User: Clicks "Summarize"
LexoraAI: Generates section-wise summary of all documents:
  - Leave Policy: 20 days annual...
  - Attendance: 85% required...
  - Work Hours: 9 AM - 6 PM...
```

---

## ğŸš€ Performance Metrics

- **Upload Speed**: 1-2 seconds for 10MB PDF
- **Search Speed**: <500ms per query
- **Indexing Speed**: ~100 chunks/second
- **Vector Dimension**: 384 (all-MiniLM-L6-v2)
- **Max Tokens**: ~4000 per response

---

## ğŸ“ Development Notes

### Adding New Features
1. Backend changes â†’ Restart backend (auto-reload enabled)
2. Frontend changes â†’ Auto-refresh (Vite HMR)

### Disabling Features
- Q&A: Comment out `/ask` endpoint
- Summarization: Comment out `/summarize` endpoint
- Upload: Comment out `/upload-document` endpoint

### Extending Document Support
Add to `loaders/` and update `api/app.py` file type check:
```python
if not (file.filename.endswith((".pdf", ".txt", ".docx"))):
```

---

## ğŸ“„ License

This project is open source and available for educational and commercial use.

---

## ğŸ¤ Support & Feedback

For issues or suggestions:
- Check troubleshooting section above
- Review backend logs for errors
- Ensure Groq API key is valid

---

## ğŸ‰ Getting Started

```bash
# 1. Clone and setup
git clone <repo>
cd Policy-Assistant

# 2. Setup backend
cd rag-backend
pip install -r requirements.txt
echo "GROQ_API_KEY=your_key" > .env

# 3. Setup frontend
cd ../rag-frontend
npm install

# 4. Run in two terminals
# Terminal 1: Backend
cd rag-backend && python -m uvicorn api.app:app --host 127.0.0.1 --port 8001

# Terminal 2: Frontend
cd rag-frontend && npm run dev

# 5. Open http://localhost:5173
```

---

**Made with â¤ï¸ by LexoraAI Team**

